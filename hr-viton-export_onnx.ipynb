{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "52fc81c9-9e51-4482-888a-43fc6b45901e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/hr-viton/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from torchvision.utils import make_grid as make_image_grid\n",
    "from torchvision.utils import save_image\n",
    "import argparse\n",
    "import os\n",
    "import time\n",
    "from cp_dataset_test import CPDatasetTest, CPDataLoader\n",
    "\n",
    "from networks import ConditionGenerator, load_checkpoint, make_grid\n",
    "from network_generator import SPADEGenerator\n",
    "from tensorboardX import SummaryWriter\n",
    "from utils import *\n",
    "\n",
    "import torchgeometry as tgm\n",
    "from collections import OrderedDict\n",
    "\n",
    "def remove_overlap(seg_out, warped_cm):\n",
    "    \n",
    "    assert len(warped_cm.shape) == 4\n",
    "    \n",
    "    warped_cm = warped_cm - (torch.cat([seg_out[:, 1:3, :, :], seg_out[:, 5:, :, :]], dim=1)).sum(dim=1, keepdim=True) * warped_cm\n",
    "    return warped_cm\n",
    "def get_opt():\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    parser.add_argument(\"--gpu_ids\", default=\"\")\n",
    "    parser.add_argument('-j', '--workers', type=int, default=4)\n",
    "    parser.add_argument('-b', '--batch-size', type=int, default=1)\n",
    "    parser.add_argument('--fp16', action='store_true', help='use amp')\n",
    "    # Cuda availability\n",
    "    parser.add_argument('--cuda',default=False, help='cuda or cpu')\n",
    "\n",
    "    parser.add_argument('--test_name', type=str, default='test', help='test name')\n",
    "    parser.add_argument(\"--dataroot\", default=\"./data/zalando-hd-resize\")\n",
    "    parser.add_argument(\"--datamode\", default=\"test\")\n",
    "    parser.add_argument(\"--data_list\", default=\"test_pairs.txt\")\n",
    "    parser.add_argument(\"--output_dir\", type=str, default=\"./Output\")\n",
    "    parser.add_argument(\"--datasetting\", default=\"unpaired\")\n",
    "    parser.add_argument(\"--fine_width\", type=int, default=768)\n",
    "    parser.add_argument(\"--fine_height\", type=int, default=1024)\n",
    "\n",
    "    parser.add_argument('--tensorboard_dir', type=str, default='./data/zalando-hd-resize/tensorboard', help='save tensorboard infos')\n",
    "    parser.add_argument('--checkpoint_dir', type=str, default='checkpoints', help='save checkpoint infos')\n",
    "    parser.add_argument('--tocg_checkpoint', type=str, default='./eval_models/weights/v0.1/mtviton.pth', help='tocg checkpoint')\n",
    "    parser.add_argument('--gen_checkpoint', type=str, default='./eval_models/weights/v0.1/gen.pth', help='G checkpoint')\n",
    "\n",
    "    parser.add_argument(\"--tensorboard_count\", type=int, default=100)\n",
    "    parser.add_argument(\"--shuffle\", action='store_true', help='shuffle input data')\n",
    "    parser.add_argument(\"--semantic_nc\", type=int, default=13)\n",
    "    parser.add_argument(\"--output_nc\", type=int, default=13)\n",
    "    parser.add_argument('--gen_semantic_nc', type=int, default=7, help='# of input label classes without unknown class')\n",
    "    \n",
    "    # network\n",
    "    parser.add_argument(\"--warp_feature\", choices=['encoder', 'T1'], default=\"T1\")\n",
    "    parser.add_argument(\"--out_layer\", choices=['relu', 'conv'], default=\"relu\")\n",
    "    \n",
    "    # training\n",
    "    parser.add_argument(\"--clothmask_composition\", type=str, choices=['no_composition', 'detach', 'warp_grad'], default='warp_grad')\n",
    "        \n",
    "    # Hyper-parameters\n",
    "    parser.add_argument('--upsample', type=str, default='bilinear', choices=['nearest', 'bilinear'])\n",
    "    parser.add_argument('--occlusion', action='store_true', help=\"Occlusion handling\")\n",
    "\n",
    "    # generator\n",
    "    parser.add_argument('--norm_G', type=str, default='spectralaliasinstance', help='instance normalization or batch normalization')\n",
    "    parser.add_argument('--ngf', type=int, default=64, help='# of gen filters in first conv layer')\n",
    "    parser.add_argument('--init_type', type=str, default='xavier', help='network initialization [normal|xavier|kaiming|orthogonal]')\n",
    "    parser.add_argument('--init_variance', type=float, default=0.02, help='variance of the initialization distribution')\n",
    "    parser.add_argument('--num_upsampling_layers', choices=('normal', 'more', 'most'), default='most', # normal: 256, more: 512\n",
    "                        help=\"If 'more', adds upsampling layer between the two middle resnet blocks. If 'most', also add one more upsampling + resnet layer at the end of the generator\")\n",
    "\n",
    "    opt = parser.parse_args()\n",
    "    return opt\n",
    "\n",
    "def load_checkpoint_G(model, checkpoint_path,opt):\n",
    "    if not os.path.exists(checkpoint_path):\n",
    "        print(\"Invalid path!\")\n",
    "        return\n",
    "    state_dict = torch.load(checkpoint_path)\n",
    "    new_state_dict = OrderedDict([(k.replace('ace', 'alias').replace('.Spade', ''), v) for (k, v) in state_dict.items()])\n",
    "    new_state_dict._metadata = OrderedDict([(k.replace('ace', 'alias').replace('.Spade', ''), v) for (k, v) in state_dict._metadata.items()])\n",
    "    model.load_state_dict(new_state_dict, strict=True)\n",
    "    if opt.cuda :\n",
    "        model.cuda()\n",
    "\n",
    "\n",
    "\n",
    "def test(opt, test_loader, tocg, generator):\n",
    "    gauss = tgm.image.GaussianBlur((15, 15), (3, 3))\n",
    "    if opt.cuda:\n",
    "        gauss = gauss.cuda()\n",
    "    \n",
    "    # Model\n",
    "    if opt.cuda :\n",
    "        tocg.cuda()\n",
    "    tocg.eval()\n",
    "    generator.eval()\n",
    "    \n",
    "    if opt.output_dir is not None:\n",
    "        output_dir = opt.output_dir\n",
    "    else:\n",
    "        output_dir = os.path.join('./output', opt.test_name,\n",
    "                            opt.datamode, opt.datasetting, 'generator', 'output')\n",
    "    grid_dir = os.path.join('./output', opt.test_name,\n",
    "                             opt.datamode, opt.datasetting, 'generator', 'grid')\n",
    "    \n",
    "    os.makedirs(grid_dir, exist_ok=True)\n",
    "    \n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    num = 0\n",
    "    iter_start_time = time.time()\n",
    "    with torch.no_grad():\n",
    "        for inputs in test_loader.data_loader:\n",
    "\n",
    "            if opt.cuda :\n",
    "                pose_map = inputs['pose'].cuda()\n",
    "                pre_clothes_mask = inputs['cloth_mask'][opt.datasetting].cuda()\n",
    "                label = inputs['parse']\n",
    "                parse_agnostic = inputs['parse_agnostic']\n",
    "                agnostic = inputs['agnostic'].cuda()\n",
    "                clothes = inputs['cloth'][opt.datasetting].cuda() # target cloth\n",
    "                densepose = inputs['densepose'].cuda()\n",
    "                im = inputs['image']\n",
    "                input_label, input_parse_agnostic = label.cuda(), parse_agnostic.cuda()\n",
    "                pre_clothes_mask = torch.FloatTensor((pre_clothes_mask.detach().cpu().numpy() > 0.5).astype(np.float)).cuda()\n",
    "            else :\n",
    "                pose_map = inputs['pose']\n",
    "                pre_clothes_mask = inputs['cloth_mask'][opt.datasetting]\n",
    "                label = inputs['parse']\n",
    "                parse_agnostic = inputs['parse_agnostic']\n",
    "                agnostic = inputs['agnostic']\n",
    "                clothes = inputs['cloth'][opt.datasetting] # target cloth\n",
    "                densepose = inputs['densepose']\n",
    "                im = inputs['image']\n",
    "                input_label, input_parse_agnostic = label, parse_agnostic\n",
    "                pre_clothes_mask = torch.FloatTensor((pre_clothes_mask.detach().cpu().numpy() > 0.5).astype(np.float))\n",
    "\n",
    "\n",
    "\n",
    "            # down\n",
    "            pose_map_down = F.interpolate(pose_map, size=(256, 192), mode='bilinear')\n",
    "            pre_clothes_mask_down = F.interpolate(pre_clothes_mask, size=(256, 192), mode='nearest')\n",
    "            input_label_down = F.interpolate(input_label, size=(256, 192), mode='bilinear')\n",
    "            input_parse_agnostic_down = F.interpolate(input_parse_agnostic, size=(256, 192), mode='nearest')\n",
    "            agnostic_down = F.interpolate(agnostic, size=(256, 192), mode='nearest')\n",
    "            clothes_down = F.interpolate(clothes, size=(256, 192), mode='bilinear')\n",
    "            densepose_down = F.interpolate(densepose, size=(256, 192), mode='bilinear')\n",
    "\n",
    "            shape = pre_clothes_mask.shape\n",
    "            \n",
    "            # multi-task inputs\n",
    "            input1 = torch.cat([clothes_down, pre_clothes_mask_down], 1)\n",
    "            input2 = torch.cat([input_parse_agnostic_down, densepose_down], 1)\n",
    "\n",
    "            # forward\n",
    "            flow_list, fake_segmap, warped_cloth_paired, warped_clothmask_paired = tocg(opt,input1, input2)\n",
    "            \n",
    "            # warped cloth mask one hot\n",
    "            if opt.cuda :\n",
    "                warped_cm_onehot = torch.FloatTensor((warped_clothmask_paired.detach().cpu().numpy() > 0.5).astype(np.float)).cuda()\n",
    "            else :\n",
    "                warped_cm_onehot = torch.FloatTensor((warped_clothmask_paired.detach().cpu().numpy() > 0.5).astype(np.float))\n",
    "\n",
    "            if opt.clothmask_composition != 'no_composition':\n",
    "                if opt.clothmask_composition == 'detach':\n",
    "                    cloth_mask = torch.ones_like(fake_segmap)\n",
    "                    cloth_mask[:,3:4, :, :] = warped_cm_onehot\n",
    "                    fake_segmap = fake_segmap * cloth_mask\n",
    "                    \n",
    "                if opt.clothmask_composition == 'warp_grad':\n",
    "                    cloth_mask = torch.ones_like(fake_segmap)\n",
    "                    cloth_mask[:,3:4, :, :] = warped_clothmask_paired\n",
    "                    fake_segmap = fake_segmap * cloth_mask\n",
    "                    \n",
    "            # make generator input parse map\n",
    "            fake_parse_gauss = gauss(F.interpolate(fake_segmap, size=(opt.fine_height, opt.fine_width), mode='bilinear'))\n",
    "            fake_parse = fake_parse_gauss.argmax(dim=1)[:, None]\n",
    "\n",
    "            if opt.cuda :\n",
    "                old_parse = torch.FloatTensor(fake_parse.size(0), 13, opt.fine_height, opt.fine_width).zero_().cuda()\n",
    "            else:\n",
    "                old_parse = torch.FloatTensor(fake_parse.size(0), 13, opt.fine_height, opt.fine_width).zero_()\n",
    "            old_parse.scatter_(1, fake_parse, 1.0)\n",
    "\n",
    "            labels = {\n",
    "                0:  ['background',  [0]],\n",
    "                1:  ['paste',       [2, 4, 7, 8, 9, 10, 11]],\n",
    "                2:  ['upper',       [3]],\n",
    "                3:  ['hair',        [1]],\n",
    "                4:  ['left_arm',    [5]],\n",
    "                5:  ['right_arm',   [6]],\n",
    "                6:  ['noise',       [12]]\n",
    "            }\n",
    "            if opt.cuda :\n",
    "                parse = torch.FloatTensor(fake_parse.size(0), 7, opt.fine_height, opt.fine_width).zero_().cuda()\n",
    "            else:\n",
    "                parse = torch.FloatTensor(fake_parse.size(0), 7, opt.fine_height, opt.fine_width).zero_()\n",
    "            for i in range(len(labels)):\n",
    "                for label in labels[i][1]:\n",
    "                    parse[:, i] += old_parse[:, label]\n",
    "                    \n",
    "            # warped cloth\n",
    "            N, _, iH, iW = clothes.shape\n",
    "            flow = F.interpolate(flow_list[-1].permute(0, 3, 1, 2), size=(iH, iW), mode='bilinear').permute(0, 2, 3, 1)\n",
    "            flow_norm = torch.cat([flow[:, :, :, 0:1] / ((96 - 1.0) / 2.0), flow[:, :, :, 1:2] / ((128 - 1.0) / 2.0)], 3)\n",
    "            \n",
    "            grid = make_grid(N, iH, iW,opt)\n",
    "            warped_grid = grid + flow_norm\n",
    "            warped_cloth = F.grid_sample(clothes, warped_grid, padding_mode='border')\n",
    "            warped_clothmask = F.grid_sample(pre_clothes_mask, warped_grid, padding_mode='border')\n",
    "            if opt.occlusion:\n",
    "                warped_clothmask = remove_overlap(F.softmax(fake_parse_gauss, dim=1), warped_clothmask)\n",
    "                warped_cloth = warped_cloth * warped_clothmask + torch.ones_like(warped_cloth) * (1-warped_clothmask)\n",
    "            \n",
    "\n",
    "            output = generator(torch.cat((agnostic, densepose, warped_cloth), dim=1), parse)\n",
    "            # visualize\n",
    "            unpaired_names = []\n",
    "            for i in range(shape[0]):\n",
    "                grid = make_image_grid([(clothes[i].cpu() / 2 + 0.5), (pre_clothes_mask[i].cpu()).expand(3, -1, -1), visualize_segmap(parse_agnostic.cpu(), batch=i), ((densepose.cpu()[i]+1)/2),\n",
    "                                        (warped_cloth[i].cpu().detach() / 2 + 0.5), (warped_clothmask[i].cpu().detach()).expand(3, -1, -1), visualize_segmap(fake_parse_gauss.cpu(), batch=i),\n",
    "                                        (pose_map[i].cpu()/2 +0.5), (warped_cloth[i].cpu()/2 + 0.5), (agnostic[i].cpu()/2 + 0.5),\n",
    "                                        (im[i]/2 +0.5), (output[i].cpu()/2 +0.5)],\n",
    "                                        nrow=4)\n",
    "                unpaired_name = (inputs['c_name']['paired'][i].split('.')[0] + '_' + inputs['c_name'][opt.datasetting][i].split('.')[0] + '.png')\n",
    "                save_image(grid, os.path.join(grid_dir, unpaired_name))\n",
    "                unpaired_names.append(unpaired_name)\n",
    "                \n",
    "            # save output\n",
    "            save_images(output, unpaired_names, output_dir)\n",
    "                \n",
    "            num += shape[0]\n",
    "            print(num)\n",
    "\n",
    "    print(f\"Test time {time.time() - iter_start_time}\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    opt = get_opt()\n",
    "    print(opt)\n",
    "    print(\"Start to test %s!\")\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = opt.gpu_ids\n",
    "    \n",
    "    # create test dataset & loader\n",
    "    test_dataset = CPDatasetTest(opt)\n",
    "    test_loader = CPDataLoader(opt, test_dataset)\n",
    "    \n",
    "    # visualization\n",
    "    # if not os.path.exists(opt.tensorboard_dir):\n",
    "    #     os.makedirs(opt.tensorboard_dir)\n",
    "    # board = SummaryWriter(log_dir=os.path.join(opt.tensorboard_dir, opt.test_name, opt.datamode, opt.datasetting))\n",
    "\n",
    "    ## Model\n",
    "    # tocg\n",
    "    input1_nc = 4  # cloth + cloth-mask\n",
    "    input2_nc = opt.semantic_nc + 3  # parse_agnostic + densepose\n",
    "    tocg = ConditionGenerator(opt, input1_nc=input1_nc, input2_nc=input2_nc, output_nc=opt.output_nc, ngf=96, norm_layer=nn.BatchNorm2d)\n",
    "       \n",
    "    # generator\n",
    "    opt.semantic_nc = 7\n",
    "    generator = SPADEGenerator(opt, 3+3+3)\n",
    "    generator.print_network()\n",
    "       \n",
    "    # Load Checkpoint\n",
    "    load_checkpoint(tocg, opt.tocg_checkpoint,opt)\n",
    "    load_checkpoint_G(generator, opt.gen_checkpoint,opt)\n",
    "\n",
    "    # Train\n",
    "    test(opt, test_loader, tocg, generator)\n",
    "\n",
    "    print(\"Finished testing!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a1d72754-7321-4f00-90cc-0946ca2b5da8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(batch_size=1, checkpoint_dir='checkpoints', clothmask_composition='warp_grad', cuda=False, data_list='test_pairs.txt', datamode='test', dataroot='data', datasetting='unpaired', fine_height=1024, fine_width=768, fp16=False, gen_checkpoint='eval_models/weights/v0.1/gen.pth', gen_semantic_nc=7, gpu_ids='', init_type='xavier', init_variance=0.02, ngf=64, norm_G='spectralaliasinstance', num_upsampling_layers='most', occlusion=True, out_layer='relu', output_dir='./Output', output_nc=13, semantic_nc=13, shuffle=False, tensorboard_count=100, tensorboard_dir='./data/zalando-hd-resize/tensorboard', test_name='test', tocg_checkpoint='eval_models/weights/v0.1/mtviton.pth', upsample='bilinear', warp_feature='T1', workers=4)\n",
      "Network [SPADEGenerator] was created. Total number of parameters: 100.5 million. To see the architecture, do print(network).\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from torch.onnx import _export as torch_onnx_export\n",
    "\n",
    "sys.argv = ['test_generator.py', '--occlusion', '-j', '4', '--test_name', 'test', '--tocg_checkpoint', 'eval_models/weights/v0.1/mtviton.pth', '--gen_checkpoint', 'eval_models/weights/v0.1/gen.pth', '--datasetting', 'unpaired', '--dataroot', 'data', '--data_list', 'test_pairs.txt']\n",
    "opt = get_opt()\n",
    "\n",
    "## Model\n",
    "# tocg\n",
    "input1_nc = 4  # cloth + cloth-mask\n",
    "input2_nc = opt.semantic_nc + 3  # parse_agnostic + densepose\n",
    "tocg = ConditionGenerator(opt, input1_nc=input1_nc, input2_nc=input2_nc, output_nc=opt.output_nc, ngf=96, norm_layer=nn.BatchNorm2d)\n",
    "   \n",
    "# generator\n",
    "opt.semantic_nc = 7\n",
    "generator = SPADEGenerator(opt, 3+3+3)\n",
    "generator.print_network()\n",
    "   \n",
    "# Load Checkpoint\n",
    "load_checkpoint(tocg, opt.tocg_checkpoint,opt)\n",
    "load_checkpoint_G(generator, opt.gen_checkpoint,opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1d3fda6e-8362-4eba-b5dd-92148cdf4c1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_8468/603680497.py:5: FutureWarning: 'torch.onnx._export' is deprecated in version 1.12.0 and will be removed in version 1.14. Please use `torch.onnx.export` instead.\n",
      "  torch_onnx_export(generator, (torch.randn((1, 9, 1024, 768)).to(\"cpu\"), torch.randn((1, 7, 1024, 768)).to(\"cpu\")), onnx_path, opset_version=13,\n",
      "/root/miniconda3/envs/hr-viton/lib/python3.8/site-packages/torch/onnx/symbolic_helper.py:1457: UserWarning: ONNX export mode is set to TrainingMode.EVAL, but operator 'instance_norm' is set to train=True. Exporting with train=True.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "generator.to(\"cpu\")\n",
    "onnx_path = \"eval_models/weights/v0.1/SPADEGenerator.onnx\"\n",
    "\n",
    "with torch.no_grad():\n",
    "    torch_onnx_export(generator, (torch.randn((1, 9, 1024, 768)).to(\"cpu\"), torch.randn((1, 7, 1024, 768)).to(\"cpu\")), onnx_path, opset_version=13,\n",
    "                      input_names=[\"x\", \"seg\"],\n",
    "                      output_names=[\"y\"], \n",
    "                      onnx_shape_inference=False,\n",
    "                      export_params=True, \n",
    "                      operator_export_type=torch.onnx.OperatorExportTypes.ONNX_ATEN_FALLBACK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aeb31fb1-1664-40bf-bf29-dac15c4da4d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_8468/125202871.py:5: FutureWarning: 'torch.onnx._export' is deprecated in version 1.12.0 and will be removed in version 1.14. Please use `torch.onnx.export` instead.\n",
      "  torch_onnx_export(tocg, (torch.randn((1, 4, 256, 192)).to(\"cpu\"), torch.randn((1, 16, 256, 192)).to(\"cpu\")), onnx_path, opset_version=13,\n",
      "/root/miniconda3/envs/hr-viton/lib/python3.8/site-packages/torch/nn/functional.py:4227: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/envs/hr-viton/lib/python3.8/site-packages/torch/onnx/symbolic_helper.py:710: UserWarning: Type cannot be inferred, which might cause exported graph to produce incorrect results.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/envs/hr-viton/lib/python3.8/site-packages/torch/onnx/utils.py:1871: FutureWarning: 'torch.onnx._patch_torch._aten_op' is deprecated in version 1.13 and will be removed in version 1.14. Please note 'g.at()' is to be removed from torch.Graph. Please open a GitHub issue if you need this functionality..\n",
      "  return graph_context.at(\n",
      "/root/miniconda3/envs/hr-viton/lib/python3.8/site-packages/torch/onnx/_patch_torch.py:108: FutureWarning: 'torch.onnx._patch_torch._graph_op' is deprecated in version 1.13 and will be removed in version 1.14. Please note 'g.op()' is to be removed from torch.Graph. Please open a GitHub issue if you need this functionality..\n",
      "  return _graph_op(\n"
     ]
    }
   ],
   "source": [
    "tocg.to(\"cpu\")\n",
    "onnx_path = \"eval_models/weights/v0.1/ConditionGenerator.onnx\"\n",
    "\n",
    "with torch.no_grad():\n",
    "    torch_onnx_export(tocg, (torch.randn((1, 4, 256, 192)).to(\"cpu\"), torch.randn((1, 16, 256, 192)).to(\"cpu\")), onnx_path, opset_version=13,\n",
    "                      input_names=[\"input1\", \"input2\"],\n",
    "                      output_names=[\"flow_list\", \"x\", \"warped_c\", \"warped_cm\"], \n",
    "                      onnx_shape_inference=False, \n",
    "                      export_params=True, \n",
    "                      operator_export_type=torch.onnx.OperatorExportTypes.ONNX_ATEN_FALLBACK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b0ea77f3-de97-4780-8534-0d7226d18974",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from torchvision.utils import make_grid\n",
    "from networks import make_grid as mkgrid\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import time\n",
    "from cp_dataset import CPDataset, CPDatasetTest, CPDataLoader\n",
    "from networks import ConditionGenerator, VGGLoss, GANLoss, load_checkpoint, save_checkpoint, define_D\n",
    "from tqdm import tqdm\n",
    "from tensorboardX import SummaryWriter\n",
    "from utils import *\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "\n",
    "def iou_metric(y_pred_batch, y_true_batch):\n",
    "    B = y_pred_batch.shape[0]\n",
    "    iou = 0\n",
    "    for i in range(B):\n",
    "        y_pred = y_pred_batch[i]\n",
    "        y_true = y_true_batch[i]\n",
    "        # y_pred is not one-hot, so need to threshold it\n",
    "        y_pred = y_pred > 0.5\n",
    "        \n",
    "        y_pred = y_pred.flatten()\n",
    "        y_true = y_true.flatten()\n",
    "\n",
    "    \n",
    "        intersection = torch.sum(y_pred[y_true == 1])\n",
    "        union = torch.sum(y_pred) + torch.sum(y_true)\n",
    "\n",
    "    \n",
    "        iou += (intersection + 1e-7) / (union - intersection + 1e-7) / B\n",
    "    return iou\n",
    "\n",
    "def remove_overlap(seg_out, warped_cm):\n",
    "    \n",
    "    assert len(warped_cm.shape) == 4\n",
    "    \n",
    "    warped_cm = warped_cm - (torch.cat([seg_out[:, 1:3, :, :], seg_out[:, 5:, :, :]], dim=1)).sum(dim=1, keepdim=True) * warped_cm\n",
    "    return warped_cm\n",
    "\n",
    "def get_opt():\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    parser.add_argument(\"--name\", default=\"test\")\n",
    "    parser.add_argument(\"--gpu_ids\", default=\"\")\n",
    "    parser.add_argument('-j', '--workers', type=int, default=4)\n",
    "    parser.add_argument('-b', '--batch-size', type=int, default=8)\n",
    "    parser.add_argument('--fp16', action='store_true', help='use amp')\n",
    "\n",
    "    parser.add_argument(\"--dataroot\", default=\"./data/\")\n",
    "    parser.add_argument(\"--datamode\", default=\"train\")\n",
    "    parser.add_argument(\"--data_list\", default=\"train_pairs.txt\")\n",
    "    parser.add_argument(\"--fine_width\", type=int, default=192)\n",
    "    parser.add_argument(\"--fine_height\", type=int, default=256)\n",
    "\n",
    "    parser.add_argument('--tensorboard_dir', type=str, default='tensorboard', help='save tensorboard infos')\n",
    "    parser.add_argument('--checkpoint_dir', type=str, default='checkpoints', help='save checkpoint infos')\n",
    "    parser.add_argument('--tocg_checkpoint', type=str, default='', help='tocg checkpoint')\n",
    "\n",
    "    parser.add_argument(\"--tensorboard_count\", type=int, default=100)\n",
    "    parser.add_argument(\"--display_count\", type=int, default=100)\n",
    "    parser.add_argument(\"--save_count\", type=int, default=10000)\n",
    "    parser.add_argument(\"--load_step\", type=int, default=0)\n",
    "    parser.add_argument(\"--keep_step\", type=int, default=300000)\n",
    "    parser.add_argument(\"--shuffle\", action='store_true', help='shuffle input data')\n",
    "    parser.add_argument(\"--semantic_nc\", type=int, default=13)\n",
    "    parser.add_argument(\"--output_nc\", type=int, default=13)\n",
    "    \n",
    "    # network\n",
    "    parser.add_argument(\"--warp_feature\", choices=['encoder', 'T1'], default=\"T1\")\n",
    "    parser.add_argument(\"--out_layer\", choices=['relu', 'conv'], default=\"relu\")\n",
    "    parser.add_argument('--Ddownx2', action='store_true', help=\"Downsample D's input to increase the receptive field\")  \n",
    "    parser.add_argument('--Ddropout', action='store_true', help=\"Apply dropout to D\")\n",
    "    parser.add_argument('--num_D', type=int, default=2, help='Generator ngf')\n",
    "    # Cuda availability\n",
    "    parser.add_argument('--cuda',default=False, help='cuda or cpu')\n",
    "    # training\n",
    "    parser.add_argument(\"--G_D_seperate\", action='store_true')\n",
    "    parser.add_argument(\"--no_GAN_loss\", action='store_true')\n",
    "    parser.add_argument(\"--lasttvonly\", action='store_true')\n",
    "    parser.add_argument(\"--interflowloss\", action='store_true', help=\"Intermediate flow loss\")\n",
    "    parser.add_argument(\"--clothmask_composition\", type=str, choices=['no_composition', 'detach', 'warp_grad'], default='warp_grad')\n",
    "    parser.add_argument('--edgeawaretv', type=str, choices=['no_edge', 'last_only', 'weighted'], default=\"no_edge\", help=\"Edge aware TV loss\")\n",
    "    parser.add_argument('--add_lasttv', action='store_true')\n",
    "    \n",
    "    # test visualize\n",
    "    parser.add_argument(\"--no_test_visualize\", action='store_true')    \n",
    "    parser.add_argument(\"--num_test_visualize\", type=int, default=3)\n",
    "    parser.add_argument(\"--test_datasetting\", default=\"unpaired\")\n",
    "    parser.add_argument(\"--test_dataroot\", default=\"./data/\")\n",
    "    parser.add_argument(\"--test_data_list\", default=\"test_pairs.txt\")\n",
    "    \n",
    "\n",
    "    # Hyper-parameters\n",
    "    parser.add_argument('--G_lr', type=float, default=0.0002, help='Generator initial learning rate for adam')\n",
    "    parser.add_argument('--D_lr', type=float, default=0.0002, help='Discriminator initial learning rate for adam')\n",
    "    parser.add_argument('--CElamda', type=float, default=10, help='initial learning rate for adam')\n",
    "    parser.add_argument('--GANlambda', type=float, default=1)\n",
    "    parser.add_argument('--tvlambda', type=float, default=2)\n",
    "    parser.add_argument('--upsample', type=str, default='bilinear', choices=['nearest', 'bilinear'])\n",
    "    parser.add_argument('--val_count', type=int, default='1000')\n",
    "    parser.add_argument('--spectral', action='store_true', help=\"Apply spectral normalization to D\")\n",
    "    parser.add_argument('--occlusion', action='store_true', help=\"Occlusion handling\")\n",
    "    \n",
    "    opt = parser.parse_args()\n",
    "    return opt\n",
    "\n",
    "\n",
    "def train(opt, train_loader, test_loader, val_loader, board, tocg, D):\n",
    "    # Model\n",
    "    tocg.cuda()\n",
    "    tocg.train()\n",
    "    D.cuda()\n",
    "    D.train()\n",
    "\n",
    "    # criterion\n",
    "    criterionL1 = nn.L1Loss()\n",
    "    criterionVGG = VGGLoss(opt)\n",
    "    if opt.fp16:\n",
    "        criterionGAN = GANLoss(use_lsgan=True, tensor=torch.cuda.HalfTensor)\n",
    "    else :\n",
    "        criterionGAN = GANLoss(use_lsgan=True, tensor=torch.cuda.FloatTensor if opt.gpu_ids else torch.Tensor)\n",
    "\n",
    "    # optimizer\n",
    "    optimizer_G = torch.optim.Adam(tocg.parameters(), lr=opt.G_lr, betas=(0.5, 0.999))\n",
    "    optimizer_D = torch.optim.Adam(D.parameters(), lr=opt.D_lr, betas=(0.5, 0.999))\n",
    "    \n",
    "\n",
    "    for step in tqdm(range(opt.load_step, opt.keep_step)):\n",
    "        iter_start_time = time.time()\n",
    "        inputs = train_loader.next_batch()\n",
    "\n",
    "        # input1\n",
    "        c_paired = inputs['cloth']['paired'].cuda()\n",
    "        cm_paired = inputs['cloth_mask']['paired'].cuda()\n",
    "        cm_paired = torch.FloatTensor((cm_paired.detach().cpu().numpy() > 0.5).astype(np.float)).cuda()\n",
    "        # input2\n",
    "        parse_agnostic = inputs['parse_agnostic'].cuda()\n",
    "        densepose = inputs['densepose'].cuda()\n",
    "        openpose = inputs['pose'].cuda()\n",
    "        # GT\n",
    "        label_onehot = inputs['parse_onehot'].cuda()  # CE\n",
    "        label = inputs['parse'].cuda()  # GAN loss\n",
    "        parse_cloth_mask = inputs['pcm'].cuda()  # L1\n",
    "        im_c = inputs['parse_cloth'].cuda()  # VGG\n",
    "        # visualization\n",
    "        im = inputs['image']\n",
    "\n",
    "        # inputs\n",
    "        input1 = torch.cat([c_paired, cm_paired], 1)\n",
    "        input2 = torch.cat([parse_agnostic, densepose], 1)\n",
    "\n",
    "        # forward\n",
    "        flow_list, fake_segmap, warped_cloth_paired, warped_clothmask_paired = tocg(input1, input2)\n",
    "        \n",
    "        # warped cloth mask one hot \n",
    "        \n",
    "        warped_cm_onehot = torch.FloatTensor((warped_clothmask_paired.detach().cpu().numpy() > 0.5).astype(np.float)).cuda()\n",
    "        # fake segmap cloth channel * warped clothmask\n",
    "        if opt.clothmask_composition != 'no_composition':\n",
    "            if opt.clothmask_composition == 'detach':\n",
    "                cloth_mask = torch.ones_like(fake_segmap.detach())\n",
    "                cloth_mask[:, 3:4, :, :] = warped_cm_onehot\n",
    "                fake_segmap = fake_segmap * cloth_mask\n",
    "                \n",
    "            if opt.clothmask_composition == 'warp_grad':\n",
    "                cloth_mask = torch.ones_like(fake_segmap.detach())\n",
    "                cloth_mask[:, 3:4, :, :] = warped_clothmask_paired\n",
    "                fake_segmap = fake_segmap * cloth_mask\n",
    "        if opt.occlusion:\n",
    "            warped_clothmask_paired = remove_overlap(F.softmax(fake_segmap, dim=1), warped_clothmask_paired)\n",
    "            warped_cloth_paired = warped_cloth_paired * warped_clothmask_paired + torch.ones_like(warped_cloth_paired) * (1-warped_clothmask_paired)\n",
    "        \n",
    "        # generated fake cloth mask & misalign mask\n",
    "        fake_clothmask = (torch.argmax(fake_segmap.detach(), dim=1, keepdim=True) == 3).long()\n",
    "        misalign = fake_clothmask - warped_cm_onehot\n",
    "        misalign[misalign < 0.0] = 0.0\n",
    "        \n",
    "        # loss warping\n",
    "        loss_l1_cloth = criterionL1(warped_clothmask_paired, parse_cloth_mask)\n",
    "        loss_vgg = criterionVGG(warped_cloth_paired, im_c)\n",
    "\n",
    "        loss_tv = 0\n",
    "        \n",
    "        if opt.edgeawaretv == 'no_edge':\n",
    "            if not opt.lasttvonly:\n",
    "                for flow in flow_list:\n",
    "                    y_tv = torch.abs(flow[:, 1:, :, :] - flow[:, :-1, :, :]).mean()\n",
    "                    x_tv = torch.abs(flow[:, :, 1:, :] - flow[:, :, :-1, :]).mean()\n",
    "                    loss_tv = loss_tv + y_tv + x_tv\n",
    "            else:\n",
    "                for flow in flow_list[-1:]:\n",
    "                    y_tv = torch.abs(flow[:, 1:, :, :] - flow[:, :-1, :, :]).mean()\n",
    "                    x_tv = torch.abs(flow[:, :, 1:, :] - flow[:, :, :-1, :]).mean()\n",
    "                    loss_tv = loss_tv + y_tv + x_tv\n",
    "        else:\n",
    "            if opt.edgeawaretv == 'last_only':\n",
    "                flow = flow_list[-1]\n",
    "                warped_clothmask_paired_down = F.interpolate(warped_clothmask_paired, flow.shape[1:3], mode='bilinear')\n",
    "                y_tv = torch.abs(flow[:, 1:, :, :] - flow[:, :-1, :, :])\n",
    "                x_tv = torch.abs(flow[:, :, 1:, :] - flow[:, :, :-1, :])\n",
    "                mask_y = torch.exp(-150*torch.abs(warped_clothmask_paired_down.permute(0, 2, 3, 1)[:, 1:, :, :] - warped_clothmask_paired_down.permute(0, 2, 3, 1)[:, :-1, :, :]))\n",
    "                mask_x = torch.exp(-150*torch.abs(warped_clothmask_paired_down.permute(0, 2, 3, 1)[:, :, 1:, :] - warped_clothmask_paired_down.permute(0, 2, 3, 1)[:, :, :-1, :]))\n",
    "                y_tv = y_tv * mask_y\n",
    "                x_tv = x_tv * mask_x\n",
    "                y_tv = y_tv.mean()\n",
    "                x_tv = x_tv.mean()\n",
    "                loss_tv = loss_tv + y_tv + x_tv\n",
    "                \n",
    "            elif opt.edgeawaretv == 'weighted':\n",
    "                for i in range(5):\n",
    "                    flow = flow_list[i]\n",
    "                    warped_clothmask_paired_down = F.interpolate(warped_clothmask_paired, flow.shape[1:3], mode='bilinear')\n",
    "                    y_tv = torch.abs(flow[:, 1:, :, :] - flow[:, :-1, :, :])\n",
    "                    x_tv = torch.abs(flow[:, :, 1:, :] - flow[:, :, :-1, :])\n",
    "                    mask_y = torch.exp(-150*torch.abs(warped_clothmask_paired_down.permute(0, 2, 3, 1)[:, 1:, :, :] - warped_clothmask_paired_down.permute(0, 2, 3, 1)[:, :-1, :, :]))\n",
    "                    mask_x = torch.exp(-150*torch.abs(warped_clothmask_paired_down.permute(0, 2, 3, 1)[:, :, 1:, :] - warped_clothmask_paired_down.permute(0, 2, 3, 1)[:, :, :-1, :]))\n",
    "                    y_tv = y_tv * mask_y\n",
    "                    x_tv = x_tv * mask_x\n",
    "                    y_tv = y_tv.mean() / (2 ** (4-i))\n",
    "                    x_tv = x_tv.mean() / (2 ** (4-i))\n",
    "                    loss_tv = loss_tv + y_tv + x_tv\n",
    "            \n",
    "            if opt.add_lasttv:\n",
    "                for flow in flow_list[-1:]:\n",
    "                    y_tv = torch.abs(flow[:, 1:, :, :] - flow[:, :-1, :, :]).mean()\n",
    "                    x_tv = torch.abs(flow[:, :, 1:, :] - flow[:, :, :-1, :]).mean()\n",
    "                    loss_tv = loss_tv + y_tv + x_tv\n",
    "            \n",
    "\n",
    "        N, _, iH, iW = c_paired.size()\n",
    "        # Intermediate flow loss\n",
    "        if opt.interflowloss:\n",
    "            for i in range(len(flow_list)-1):\n",
    "                flow = flow_list[i]\n",
    "                N, fH, fW, _ = flow.size()\n",
    "                grid = mkgrid(N, iH, iW)\n",
    "                flow = F.interpolate(flow.permute(0, 3, 1, 2), size = c_paired.shape[2:], mode=opt.upsample).permute(0, 2, 3, 1)\n",
    "                flow_norm = torch.cat([flow[:, :, :, 0:1] / ((fW - 1.0) / 2.0), flow[:, :, :, 1:2] / ((fH - 1.0) / 2.0)], 3)\n",
    "                warped_c = F.grid_sample(c_paired, flow_norm + grid, padding_mode='border')\n",
    "                warped_cm = F.grid_sample(cm_paired, flow_norm + grid, padding_mode='border')\n",
    "                warped_cm = remove_overlap(F.softmax(fake_segmap, dim=1), warped_cm)\n",
    "                loss_l1_cloth += criterionL1(warped_cm, parse_cloth_mask) / (2 ** (4-i))\n",
    "                loss_vgg += criterionVGG(warped_c, im_c) / (2 ** (4-i))\n",
    "            \n",
    "        # loss segmentation\n",
    "        # generator\n",
    "        CE_loss = cross_entropy2d(fake_segmap, label_onehot.transpose(0, 1)[0].long())\n",
    "        \n",
    "        if opt.no_GAN_loss:\n",
    "            loss_G = (10 * loss_l1_cloth + loss_vgg + opt.tvlambda * loss_tv) + (CE_loss * opt.CElamda)\n",
    "            # step\n",
    "            optimizer_G.zero_grad()\n",
    "            loss_G.backward()\n",
    "            optimizer_G.step()\n",
    "        \n",
    "        else:\n",
    "            fake_segmap_softmax = torch.softmax(fake_segmap, 1)\n",
    "\n",
    "            pred_segmap = D(torch.cat((input1.detach(), input2.detach(), fake_segmap_softmax), dim=1))\n",
    "            \n",
    "            loss_G_GAN = criterionGAN(pred_segmap, True)\n",
    "            \n",
    "            if not opt.G_D_seperate:  \n",
    "                # discriminator\n",
    "                fake_segmap_pred = D(torch.cat((input1.detach(), input2.detach(), fake_segmap_softmax.detach()),dim=1))\n",
    "                real_segmap_pred = D(torch.cat((input1.detach(), input2.detach(), label),dim=1))\n",
    "                loss_D_fake = criterionGAN(fake_segmap_pred, False)\n",
    "                loss_D_real = criterionGAN(real_segmap_pred, True)\n",
    "\n",
    "                # loss sum\n",
    "                loss_G = (10 * loss_l1_cloth + loss_vgg +opt.tvlambda * loss_tv) + (CE_loss * opt.CElamda + loss_G_GAN * opt.GANlambda)  # warping + seg_generation\n",
    "                loss_D = loss_D_fake + loss_D_real\n",
    "\n",
    "                # step\n",
    "                optimizer_G.zero_grad()\n",
    "                loss_G.backward()\n",
    "                optimizer_G.step()\n",
    "                \n",
    "                optimizer_D.zero_grad()\n",
    "                loss_D.backward()\n",
    "                optimizer_D.step()\n",
    "                \n",
    "            else: # train G first after that train D\n",
    "                # loss G sum\n",
    "                loss_G = (10 * loss_l1_cloth + loss_vgg + opt.tvlambda * loss_tv) + (CE_loss * opt.CElamda + loss_G_GAN * opt.GANlambda)  # warping + seg_generation\n",
    "                \n",
    "                # step G\n",
    "                optimizer_G.zero_grad()\n",
    "                loss_G.backward()\n",
    "                optimizer_G.step()\n",
    "                \n",
    "                # discriminator\n",
    "                with torch.no_grad():\n",
    "                    _, fake_segmap, _, _ = tocg(input1, input2)\n",
    "                fake_segmap_softmax = torch.softmax(fake_segmap, 1)\n",
    "                \n",
    "                # loss discriminator\n",
    "                fake_segmap_pred = D(torch.cat((input1.detach(), input2.detach(), fake_segmap_softmax.detach()),dim=1))\n",
    "                real_segmap_pred = D(torch.cat((input1.detach(), input2.detach(), label),dim=1))\n",
    "                loss_D_fake = criterionGAN(fake_segmap_pred, False)\n",
    "                loss_D_real = criterionGAN(real_segmap_pred, True)\n",
    "                \n",
    "                loss_D = loss_D_fake + loss_D_real\n",
    "                \n",
    "                optimizer_D.zero_grad()\n",
    "                loss_D.backward()\n",
    "                optimizer_D.step()\n",
    "        # Vaildation\n",
    "        if (step + 1) % opt.val_count == 0:\n",
    "            tocg.eval()\n",
    "            iou_list = []\n",
    "            with torch.no_grad():\n",
    "                for cnt in range(2000//opt.batch_size):\n",
    "                \n",
    "                    inputs = val_loader.next_batch()\n",
    "                    # input1\n",
    "                    c_paired = inputs['cloth']['paired'].cuda()\n",
    "                    cm_paired = inputs['cloth_mask']['paired'].cuda()\n",
    "                    cm_paired = torch.FloatTensor((cm_paired.detach().cpu().numpy() > 0.5).astype(np.float)).cuda()\n",
    "                    # input2\n",
    "                    parse_agnostic = inputs['parse_agnostic'].cuda()\n",
    "                    densepose = inputs['densepose'].cuda()\n",
    "                    openpose = inputs['pose'].cuda()\n",
    "                    # GT\n",
    "                    label_onehot = inputs['parse_onehot'].cuda()  # CE\n",
    "                    label = inputs['parse'].cuda()  # GAN loss\n",
    "                    parse_cloth_mask = inputs['pcm'].cuda()  # L1\n",
    "                    im_c = inputs['parse_cloth'].cuda()  # VGG\n",
    "                    # visualization\n",
    "                    im = inputs['image']\n",
    "                    \n",
    "                    input1 = torch.cat([c_paired, cm_paired], 1)\n",
    "                    input2 = torch.cat([parse_agnostic, densepose], 1)\n",
    "                    \n",
    "                    # forward\n",
    "                    flow_list, fake_segmap, warped_cloth_paired, warped_clothmask_paired = tocg(input1, input2)\n",
    "                \n",
    "                    # fake segmap cloth channel * warped clothmask\n",
    "                    if opt.clothmask_composition != 'no_composition':\n",
    "                        if opt.clothmask_composition == 'detach':\n",
    "                            cloth_mask = torch.ones_like(fake_segmap.detach())\n",
    "                            cloth_mask[:, 3:4, :, :] = warped_cm_onehot\n",
    "                            fake_segmap = fake_segmap * cloth_mask\n",
    "                            \n",
    "                        if opt.clothmask_composition == 'warp_grad':\n",
    "                            cloth_mask = torch.ones_like(fake_segmap.detach())\n",
    "                            cloth_mask[:, 3:4, :, :] = warped_clothmask_paired\n",
    "                            fake_segmap = fake_segmap * cloth_mask\n",
    "    \n",
    "                    # calculate iou\n",
    "                    iou = iou_metric(F.softmax(fake_segmap, dim=1).detach(), label)\n",
    "                    iou_list.append(iou.item())\n",
    "\n",
    "            tocg.train()\n",
    "            board.add_scalar('val/iou', np.mean(iou_list), step + 1)\n",
    "        \n",
    "        # tensorboard\n",
    "        if (step + 1) % opt.tensorboard_count == 0:\n",
    "            # loss G\n",
    "            board.add_scalar('Loss/G', loss_G.item(), step + 1)\n",
    "            board.add_scalar('Loss/G/l1_cloth', loss_l1_cloth.item(), step + 1)\n",
    "            board.add_scalar('Loss/G/vgg', loss_vgg.item(), step + 1)\n",
    "            board.add_scalar('Loss/G/tv', loss_tv.item(), step + 1)\n",
    "            board.add_scalar('Loss/G/CE', CE_loss.item(), step + 1)\n",
    "            if not opt.no_GAN_loss:\n",
    "                board.add_scalar('Loss/G/GAN', loss_G_GAN.item(), step + 1)\n",
    "                # loss D\n",
    "                board.add_scalar('Loss/D', loss_D.item(), step + 1)\n",
    "                board.add_scalar('Loss/D/pred_real', loss_D_real.item(), step + 1)\n",
    "                board.add_scalar('Loss/D/pred_fake', loss_D_fake.item(), step + 1)\n",
    "            \n",
    "            grid = make_grid([(c_paired[0].cpu() / 2 + 0.5), (cm_paired[0].cpu()).expand(3, -1, -1), visualize_segmap(parse_agnostic.cpu()), ((densepose.cpu()[0]+1)/2),\n",
    "                              (im_c[0].cpu() / 2 + 0.5), parse_cloth_mask[0].cpu().expand(3, -1, -1), (warped_cloth_paired[0].cpu().detach() / 2 + 0.5), (warped_cm_onehot[0].cpu().detach()).expand(3, -1, -1),\n",
    "                              visualize_segmap(label.cpu()), visualize_segmap(fake_segmap.cpu()), (im[0]/2 +0.5), (misalign[0].cpu().detach()).expand(3, -1, -1)],\n",
    "                                nrow=4)\n",
    "            board.add_images('train_images', grid.unsqueeze(0), step + 1)\n",
    "            \n",
    "            if not opt.no_test_visualize:\n",
    "                inputs = test_loader.next_batch()\n",
    "                # input1\n",
    "                c_paired = inputs['cloth'][opt.test_datasetting].cuda()\n",
    "                cm_paired = inputs['cloth_mask'][opt.test_datasetting].cuda()\n",
    "                cm_paired = torch.FloatTensor((cm_paired.detach().cpu().numpy() > 0.5).astype(np.float)).cuda()\n",
    "                # input2\n",
    "                parse_agnostic = inputs['parse_agnostic'].cuda()\n",
    "                densepose = inputs['densepose'].cuda()\n",
    "                openpose = inputs['pose'].cuda()\n",
    "                # GT\n",
    "                label_onehot = inputs['parse_onehot'].cuda()  # CE\n",
    "                label = inputs['parse'].cuda()  # GAN loss\n",
    "                parse_cloth_mask = inputs['pcm'].cuda()  # L1\n",
    "                im_c = inputs['parse_cloth'].cuda()  # VGG\n",
    "                # visualization\n",
    "                im = inputs['image']\n",
    "\n",
    "                tocg.eval()\n",
    "                with torch.no_grad():\n",
    "                    # inputs\n",
    "                    input1 = torch.cat([c_paired, cm_paired], 1)\n",
    "                    input2 = torch.cat([parse_agnostic, densepose], 1)\n",
    "\n",
    "                    # forward\n",
    "                    flow_list, fake_segmap, warped_cloth_paired, warped_clothmask_paired = tocg(input1, input2)\n",
    "                    \n",
    "                    warped_cm_onehot = torch.FloatTensor((warped_clothmask_paired.detach().cpu().numpy() > 0.5).astype(np.float)).cuda()\n",
    "                    if opt.clothmask_composition != 'no_composition':\n",
    "                        if opt.clothmask_composition == 'detach':\n",
    "                            cloth_mask = torch.ones_like(fake_segmap)\n",
    "                            cloth_mask[:,3:4, :, :] = warped_cm_onehot\n",
    "                            fake_segmap = fake_segmap * cloth_mask\n",
    "                            \n",
    "                        if opt.clothmask_composition == 'warp_grad':\n",
    "                            cloth_mask = torch.ones_like(fake_segmap)\n",
    "                            cloth_mask[:,3:4, :, :] = warped_clothmask_paired\n",
    "                            fake_segmap = fake_segmap * cloth_mask\n",
    "                    if opt.occlusion:\n",
    "                        warped_clothmask_paired = remove_overlap(F.softmax(fake_segmap, dim=1), warped_clothmask_paired)\n",
    "                        warped_cloth_paired = warped_cloth_paired * warped_clothmask_paired + torch.ones_like(warped_cloth_paired) * (1-warped_clothmask_paired)\n",
    "                    \n",
    "                    # generated fake cloth mask & misalign mask\n",
    "                    fake_clothmask = (torch.argmax(fake_segmap.detach(), dim=1, keepdim=True) == 3).long()\n",
    "                    misalign = fake_clothmask - warped_cm_onehot\n",
    "                    misalign[misalign < 0.0] = 0.0\n",
    "                \n",
    "                for i in range(opt.num_test_visualize):\n",
    "                    grid = make_grid([(c_paired[i].cpu() / 2 + 0.5), (cm_paired[i].cpu()).expand(3, -1, -1), visualize_segmap(parse_agnostic.cpu(), batch=i), ((densepose.cpu()[i]+1)/2),\n",
    "                                    (im_c[i].cpu() / 2 + 0.5), parse_cloth_mask[i].cpu().expand(3, -1, -1), (warped_cloth_paired[i].cpu().detach() / 2 + 0.5), (warped_cm_onehot[i].cpu().detach()).expand(3, -1, -1),\n",
    "                                    visualize_segmap(label.cpu(), batch=i), visualize_segmap(fake_segmap.cpu(), batch=i), (im[i]/2 +0.5), (misalign[i].cpu().detach()).expand(3, -1, -1)],\n",
    "                                        nrow=4)\n",
    "                    board.add_images(f'test_images/{i}', grid.unsqueeze(0), step + 1)\n",
    "                tocg.train()\n",
    "        \n",
    "        # display\n",
    "        if (step + 1) % opt.display_count == 0:\n",
    "            t = time.time() - iter_start_time\n",
    "            if not opt.no_GAN_loss:\n",
    "                print(\"step: %8d, time: %.3f\\nloss G: %.4f, L1_cloth loss: %.4f, VGG loss: %.4f, TV loss: %.4f CE: %.4f, G GAN: %.4f\\nloss D: %.4f, D real: %.4f, D fake: %.4f\"\n",
    "                    % (step + 1, t, loss_G.item(), loss_l1_cloth.item(), loss_vgg.item(), loss_tv.item(), CE_loss.item(), loss_G_GAN.item(), loss_D.item(), loss_D_real.item(), loss_D_fake.item()), flush=True)\n",
    "\n",
    "        # save\n",
    "        if (step + 1) % opt.save_count == 0:\n",
    "            save_checkpoint(tocg, os.path.join(opt.checkpoint_dir, opt.name, 'tocg_step_%06d.pth' % (step + 1)),opt)\n",
    "            save_checkpoint(D, os.path.join(opt.checkpoint_dir, opt.name, 'D_step_%06d.pth' % (step + 1)),opt)\n",
    "\n",
    "def main():\n",
    "    opt = get_opt()\n",
    "    print(opt)\n",
    "    print(\"Start to train %s!\" % opt.name)\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = opt.gpu_ids\n",
    "    \n",
    "    # create train dataset & loader\n",
    "    train_dataset = CPDataset(opt)\n",
    "    train_loader = CPDataLoader(opt, train_dataset)\n",
    "    \n",
    "    # create test dataset & loader\n",
    "    test_loader = None\n",
    "    if not opt.no_test_visualize:\n",
    "        train_bsize = opt.batch_size\n",
    "        opt.batch_size = opt.num_test_visualize\n",
    "        opt.dataroot = opt.test_dataroot\n",
    "        opt.datamode = 'test'\n",
    "        opt.data_list = opt.test_data_list\n",
    "        test_dataset = CPDatasetTest(opt)\n",
    "        opt.batch_size = train_bsize\n",
    "        val_dataset = Subset(test_dataset, np.arange(2000))\n",
    "        test_loader = CPDataLoader(opt, test_dataset)\n",
    "        val_loader = CPDataLoader(opt, val_dataset)\n",
    "    # visualization\n",
    "    if not os.path.exists(opt.tensorboard_dir):\n",
    "        os.makedirs(opt.tensorboard_dir)\n",
    "    board = SummaryWriter(log_dir=os.path.join(opt.tensorboard_dir, opt.name))\n",
    "\n",
    "    # Model\n",
    "    input1_nc = 4  # cloth + cloth-mask\n",
    "    input2_nc = opt.semantic_nc + 3  # parse_agnostic + densepose\n",
    "    tocg = ConditionGenerator(opt, input1_nc=4, input2_nc=input2_nc, output_nc=opt.output_nc, ngf=96, norm_layer=nn.BatchNorm2d)\n",
    "    D = define_D(input_nc=input1_nc + input2_nc + opt.output_nc, Ddownx2 = opt.Ddownx2, Ddropout = opt.Ddropout, n_layers_D=3, spectral = opt.spectral, num_D = opt.num_D)\n",
    "    \n",
    "    # Load Checkpoint\n",
    "    if not opt.tocg_checkpoint == '' and os.path.exists(opt.tocg_checkpoint):\n",
    "        load_checkpoint(tocg, opt.tocg_checkpoint)\n",
    "\n",
    "    # Train\n",
    "    train(opt, train_loader, val_loader, test_loader, board, tocg, D)\n",
    "\n",
    "    # Save Checkpoint\n",
    "    save_checkpoint(tocg, os.path.join(opt.checkpoint_dir, opt.name, 'tocg_final.pth'),opt)\n",
    "    save_checkpoint(D, os.path.join(opt.checkpoint_dir, opt.name, 'D_final.pth'),opt)\n",
    "    print(\"Finished training %s!\" % opt.name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "de0bdcf4-8384-47a7-97a9-09b741411670",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.argv = ['train_condition.py', '--name', 'train', '-j', '4', '--Ddownx2', '--Ddropout', '--lasttvonly', '--interflowloss', '--occlusion', '--dataroot', 'data/zalando-hd-resize', '--test_dataroot', 'data/zalando-hd-resize']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6fe1bf88-5eae-402f-9b51-11b693baa710",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(CElamda=10, D_lr=0.0002, Ddownx2=True, Ddropout=True, GANlambda=1, G_D_seperate=False, G_lr=0.0002, add_lasttv=False, batch_size=8, checkpoint_dir='checkpoints', clothmask_composition='warp_grad', cuda=False, data_list='train_pairs.txt', datamode='train', dataroot='data/zalando-hd-resize', display_count=100, edgeawaretv='no_edge', fine_height=256, fine_width=192, fp16=False, gpu_ids='', interflowloss=True, keep_step=300000, lasttvonly=True, load_step=0, name='train', no_GAN_loss=False, no_test_visualize=False, num_D=2, num_test_visualize=3, occlusion=True, out_layer='relu', output_nc=13, save_count=10000, semantic_nc=13, shuffle=False, spectral=False, tensorboard_count=100, tensorboard_dir='tensorboard', test_data_list='test_pairs.txt', test_dataroot='data/zalando-hd-resize', test_datasetting='unpaired', tocg_checkpoint='', tvlambda=2, upsample='bilinear', val_count=1000, warp_feature='T1', workers=4)\n",
      "MultiscaleDiscriminator(\n",
      "  (layer0): Sequential(\n",
      "    (0): Conv2d(33, 64, kernel_size=(4, 4), stride=(2, 2), padding=(2, 2))\n",
      "    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (2): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(2, 2))\n",
      "    (3): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "    (4): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (5): Dropout(p=0.5, inplace=False)\n",
      "    (6): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(2, 2))\n",
      "    (7): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "    (8): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (9): Dropout(p=0.5, inplace=False)\n",
      "    (10): Conv2d(256, 512, kernel_size=(4, 4), stride=(1, 1), padding=(2, 2))\n",
      "    (11): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "    (12): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (13): Conv2d(512, 1, kernel_size=(4, 4), stride=(1, 1), padding=(2, 2))\n",
      "  )\n",
      "  (layer1): Sequential(\n",
      "    (0): Conv2d(33, 64, kernel_size=(4, 4), stride=(2, 2), padding=(2, 2))\n",
      "    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (2): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(2, 2))\n",
      "    (3): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "    (4): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (5): Dropout(p=0.5, inplace=False)\n",
      "    (6): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(2, 2))\n",
      "    (7): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "    (8): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (9): Dropout(p=0.5, inplace=False)\n",
      "    (10): Conv2d(256, 512, kernel_size=(4, 4), stride=(1, 1), padding=(2, 2))\n",
      "    (11): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "    (12): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (13): Conv2d(512, 1, kernel_size=(4, 4), stride=(1, 1), padding=(2, 2))\n",
      "  )\n",
      "  (downsample): AvgPool2d(kernel_size=3, stride=2, padding=[1, 1])\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_8468/710950412.py:13: FutureWarning: 'torch.onnx._export' is deprecated in version 1.12.0 and will be removed in version 1.14. Please use `torch.onnx.export` instead.\n",
      "  torch_onnx_export(D, (torch.randn((8, 33, 256, 19)).to(\"cpu\"),), onnx_path, opset_version=13,\n",
      "/root/miniconda3/envs/hr-viton/lib/python3.8/site-packages/torch/onnx/symbolic_helper.py:1457: UserWarning: ONNX export mode is set to TrainingMode.EVAL, but operator 'instance_norm' is set to train=True. Exporting with train=True.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "opt = get_opt()\n",
    "print(opt)\n",
    "\n",
    "input1_nc = 4  # cloth + cloth-mask\n",
    "input2_nc = opt.semantic_nc + 3  # parse_agnostic + densepose\n",
    "D = define_D(input_nc=input1_nc + input2_nc + opt.output_nc, Ddownx2 = opt.Ddownx2, Ddropout = opt.Ddropout, n_layers_D=3, spectral = opt.spectral, num_D = opt.num_D)\n",
    "load_checkpoint(D, \"eval_models/weights/v0.1/discriminator_mtviton.pth\", opt)\n",
    "\n",
    "D.to(\"cpu\")\n",
    "onnx_path = \"eval_models/weights/v0.1/discriminator_mtviton.onnx\"\n",
    "\n",
    "with torch.no_grad():\n",
    "    torch_onnx_export(D, (torch.randn((8, 33, 256, 19)).to(\"cpu\"),), onnx_path, opset_version=13,\n",
    "                      input_names=[\"input\"],\n",
    "                      output_names=[\"result\"], \n",
    "                      onnx_shape_inference=False, \n",
    "                      export_params=True, \n",
    "                      operator_export_type=torch.onnx.OperatorExportTypes.ONNX_ATEN_FALLBACK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70948c15-0ac8-4c47-bf38-a1a532c5e7e1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
